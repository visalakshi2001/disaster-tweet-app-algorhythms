# -*- coding: utf-8 -*-
"""Twitter_API_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bQ03XjKYZcfs1FQptC6XjPN6JaLVB2Ad

### With Twitter API

# tweepy with oauth v2 app_only
# """
#
# import tweepy
#
# bearer_token="---"
# access_token="---"
# access_token_secret="---"
# consumer_key = "---"
# consumer_secret = "---"
#
# client_id = "---"
# client_secret = "---"
#
#
# client = tweepy.Client(consumer_key=consumer_key,
#                        consumer_secret=consumer_secret,
#                        access_token=access_token,
#                        access_token_secret=access_token_secret,
#                        bearer_token=bearer_token
#                        )
#
#
# query = '#forestfire -is:retweet lang:en'
#
# client.get_me()
#
# # Authenticate with Twitter
# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
# auth.set_access_token(access_token, access_token_secret)
# api = tweepy.API(auth)
#
# # Define your search query
# search_query = 'your_search_query'
#
# # Fetch tweets based on the search query
# tweets = tweepy.Cursor(api.search_tweets, q=query, lang='en').items(10)
#
# # tweets = api.search_tweets(q=query, lang="en", count=10, tweet_mode ='extended')
#
#
#
#
#


"""### Nitter Scraper

"""

from ntscraper import Nitter
import pandas as pd
import numpy as np

scraper = Nitter()

# tweets_forest = scraper.get_tweets(terms='forestfire', mode='hashtag', number=200, near='usa', exclude=['nativeretweets'], language='en')

# print(tweets_forest)
# for tweet in tweets_forest['tweets']:
#     link = tweet['link']
#     print(link)

def save_tweets(tweet_dict, df = None, topic="None"):
    if df is None:
        df =  pd.DataFrame(columns=['link', 'text', 'user', 'date', 'is_retweet', 'comments', 'retweets', 'quotes', 'likes', 'images', 'video', 'gifs', "topic"])
    count = 0
    for tweet in tweet_dict['tweets']:
        count+=1
        images_ = [tweet['pictures'][0] if tweet['pictures'] != [] else np.nan]
        videos_ = [tweet['videos'][0] if tweet['videos'] != [] else np.nan]
        gifs_ = [tweet['gifs'][0] if tweet['gifs'] != [] else np.nan]
        df = df.append(pd.DataFrame([[tweet["link"],
                tweet["text"],
                tweet["user"]["name"],
                tweet['date'],
                tweet['is-retweet'],
                tweet['stats']['comments'],
                tweet['stats']['retweets'],
                tweet['stats']['quotes'],
                tweet['stats']['likes'],
                images_[0],
                videos_[0],
                gifs_[0],
                topic
                    ]], columns = df.columns))

    df.to_csv("saved_tweets.csv", index=False)

    return df

# dfForest = save_tweets(tweets_forest, topic="#forestfire")

# tweets_earthquake = scraper.get_tweets(terms='earthquake', mode='hashtag', number=200, near='usa', exclude=['nativeretweets'], language='en')
#
# df = save_tweets(tweets_forest, df=df, topic="#earthquake")
#
# df
#
# tweets_floods = scraper.get_tweets(terms='floods', mode='hashtag', number=200, near='usa', exclude=['nativeretweets'], language='en')
#
# df = save_tweets(tweets_forest, df=df, topic="#floods")
#
# tweets_hurricane = scraper.get_tweets(terms='hurricane', mode='hashtag', number=200, near='usa', exclude=['nativeretweets'], language='en')
#
# df = save_tweets(tweets_forest, df=df, topic="#hurricane")
#
# df













